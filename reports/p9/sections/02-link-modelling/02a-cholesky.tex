\subsection{The Cholesky Decomposition}\label{sec:cholesky}
In \autoref{sec:simulatingvalues}, we utilised the Cholesky decomposition on the covariance matrix in the stochastic shadow fading part. The Cholesky decomposition is a decomposition algorithm for \gls{symmetric}, \gls{pd-matrix} into the product of a \gls{lt-matrix} and its \gls{conjugate-transpose}, and is primarily used for solving systems of linear equations~\cite{Press:2007:NRE:1403886}. In this Section, we present and describe the Cholesky decomposition, as well as the problems the decomposition creates for our computation time of the stochastic shadow fading part of the link model, as well as possible ways for us to optimise our usage of the Cholesky decomposition. \autoref{algo:cholesky} contains a pseudo code description of the Cholesky decomposition. \medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwResult{The Cholesky decomposition of the input matrix}
    \SetKwFunction{Cholesky}{Cholesky}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\Cholesky{matrix, N}}{
        result $\leftarrow$ empty matrix of size N $\times$ N \;
        \For{n $\leftarrow$ 0, n < N}{
            \For{m $\leftarrow$ 0, m < n + 1}{
                sum $\leftarrow$ 0\;
                \For{i $\leftarrow$ 0, i < m}{
                    sum $\leftarrow$ sum + result$_{n,i} \cdot$ result$_{m,i}$\;
                }
                \If{n = m}{
                    \If{$\text{matrix}_{n,n} - \text{sum} \leq 0$}{
                        throw error; matrix is not positive-definite
                    }

                    results$_{n,m}$ $\leftarrow$ $\sqrt{\text{matrix}_{n,n} - \text{sum}}$\;
                }
                \Else{
                    results$_{n,m} \leftarrow \frac{1}{\text{result}_{m,m}} \cdot (\text{matrix}_{n,m} - sum)$\;
                }
            }
        }
        \KwRet result
    }
    \caption{Cholesky decomposition}
    \label{algo:cholesky}
\end{algorithm}
\medbreak
The first issue we have found with the Cholesky decomposition, or more specifically with the covariance matrix, is that the covariance matrix is not guaranteed to be a \gls{pd-matrix}. The covariance matrix is based on the relation between links in the network, which means that whether the matrix is positive-definite or not is entirely based on the network. To work around this, we have employed a tool called NearPD~\cite{website:nearPD} to transform our covariance matrix into a new matrix that has the positive-definite property, while minimising the Frobenius norm~\cite{website:frobieniusnorm} of the difference between the original and the new matrix. This significantly increases the time required to compute the link model, however, which leads us to our next major issue: The computational time required by the Cholesky decomposition itself. The computational time required for the NearPD tool and the Cholesky decomposition can be seen in \autoref{table:cholesky:spdtime}.\smallbreak

The Cholesky decomposition has a computational complexity of $O(n^3)$~\cite{Press:2007:NRE:1403886}. With a fully connected network of 1000 nodes, we would have a $\frac{1000(1000+1)}{2} - 1000 = 499500$ (\autoref{eq:lengthoflinks}) unique links, which means that our correlation (and covariance) matrix would be of size $499500 \times 499500$. This is a major issue, as we would like to be able to compute the link model in a relatively short amount of time. To combat this issue, we have two possible solutions: removing links with a distance over a certain threshold, and clustering nodes that are very close to each-other. We present the first solution in this section, and clustering in \autoref{sec:clustering}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Nodes & Links & NearPD          & Cholesky                 \\\hline
        10    & 45    & 3 milliseconds  & \textless{}1 millisecond \\\hline
        20    & 190   & 88 milliseconds & 3 milliseconds           \\\hline
        30    & 435   & 1 seconds       & 34 milliseconds          \\\hline
        40    & 780   & 6 seconds       & 200 milliseconds         \\\hline
        50    & 1225  & 32 seconds      & 720 milliseconds         \\\hline
        60    & 1770  & 113 seconds     & 3 seconds                \\\hline
        70    & 2415  & 285 seconds     & 6 seconds                \\\hline
        80    & 3160  & 584 seconds     & 12 seconds               \\\hline
        90    & 4005  & 22 minutes      & 26 seconds               \\\hline
        100   & 4950  & 35 minutes      & 45 seconds               \\\hline
        110   & 5995  & 75 minutes      & 93 seconds               \\\hline
        120   & 7140  & 127 minutes     & 155 seconds              \\\hline
        130   & 8385  & 235 minutes     & 250 seconds              \\\hline
        140   & 9730  & Timeout         & \dots                    \\\hline%582 seconds              \\\hline
        %150   & 11175 & \dots           & 14 minutes               \\\hline
        %160   & 12720 & \dots           & 17 minutes               \\\hline
        %170   & 14365 & \dots           & 20 minutes               \\\hline
        %180   & 16110 & \dots           & 29 minutes               \\\hline
        %190   & 17955 & \dots           & 39 minutes               \\\hline
        %200   & 19900 & \dots           & 52 minutes               \\\hline
    \end{tabular}
    \caption{Computation time measurement for NearPD and Cholesky decomposition.}
    \label{table:cholesky:spdtime}
\end{table}

\autoref{table:cholesky:spdtime} shows time measurements from running the cholesky decomposition computation with different network topology sizes with the NearPD tool. The measurements shows that the NearPD tool is not fast enough, as at only 130 nodes with 8385 links the tool takes 235 minutes to compute the \gls{pd-matrix}. This is not fast enough and a better solution will have to be found. The cholesky decomposition computation speed is also problematic, as the goal is 1000 nodes and already at 120 nodes the decomposition takes more than 2 minutes.

\subsubsection{Distance Threshold}\label{sec:distancethreshold}
In \autoref{sec:linkmodel}, we saw that the distance dependent \gls{pathloss} has significantly more importance in the total \gls{pathloss} than the stochastic shadow fading part. In \autoref{eq:pathlossdetermG}, we see that the distance \gls{pathloss} is 92 \acrshort{dbm} for links of 100 meters, and 100.2 \acrshort{dbm} for the diagonal links of $141.42$ meters, and in \autoref{eq:pathlossfadingG}, we see (stochastic) \gls{pathloss} values between $-13.837$ and $6.413$. This means that, entirely based on the distance part of the link model, according to the formula for computing packet error probability in \autoref{sec:pep}, with a distance of 1000 meters, and a transmission power of 26 \acrshort{dbm}, we would have a link \gls{pathloss} of $147$ \acrshort{dbm} (disregarding the stochastic shadow fading part of the \gls{pathloss}), which in turn would mean that the \gls{rssi} on the receiving end of the link would be $26 - 147 = -121$, or equivalent to a packet loss probability of 99.999~\% (assuming the noise figure and thermal noise is the same as in \autoref{sec:pep}, and the packet size is 160 bits). Hence removing links that are far away can potentially reduce the size of the correlation matrix significantly. To show this, we ran an experiment, where we generated 1000 nodes with random locations in a $25 \times 25$ kilometre area, created links between these nodes based on their location, and iteratively scaled the maximum distance allowed between nodes (the distance threshold) and calculated the link path loss, based on the distance threshold. The results of this experiment can be seen in \autoref{table:cholesky:distance-threshold}. Recall that a fully connected network topology of 1000 nodes would have a total of 499500 links. With a distance threshold of 1000 meters, we get a total of 2346 links in our $25 \times 25$ kilometre area, which is a significant improvement as we would be able to compute the Cholesky decomposition in approximately six seconds (disregarding the positive-definiteness of the correlation matrix) according to \autoref{table:cholesky:spdtime}.

%The experiment consisted of generating 1000 random nodes in a 25 kilometre square area, create links from those nodes based on their location, iteratively scale the maximum allowed distance of the links (distance threshold) and calculate the link path loss. The results can be read in \autoref{table:cholesky:distance-threshold}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Distance threshold & Links & Probability \\\hline % & Cholesky         \\\hline
        % 100 meters         & 182   & \\\hline % & 3 milliseconds   \\\hline
        % 125 meters         & 256   & \\\hline % & 7 milliseconds   \\\hline
        % 150 meters         & 360   & \\\hline % & 20 milliseconds  \\\hline
        % 175 meters         & 483   & \\\hline % & 46 milliseconds  \\\hline
        200 meters         & 92    & 0.001~\%    \\\hline % & 97 milliseconds  \\\hline
        % 225 meters         & 773   & \\\hline % & 185 milliseconds \\\hline
        250 meters         & 139   & 0.001~\%    \\\hline % & 333 milliseconds \\\hline
        % 275 meters         & 1157  & \\\hline % & 646 milliseconds \\\hline
        300 meters         & 208   & 0.001~\%    \\\hline % & 1 second         \\\hline
        % 325 meters         & 1589  & \\\hline % & 1 second         \\\hline
        350 meters         & 296   & 0.001~\%    \\\hline % & 2 seconds        \\\hline
        % 375 meters         & 2076  & \\\hline % & 4 seconds        \\\hline
        400 meters         & 394   & 0.001~\%    \\\hline % & 6 seconds        \\\hline
        % 425 meters         & 2642  & \\\hline % & 8 seconds        \\\hline
        450 meters         & 490   & 0.001~\%    \\\hline % & 11 seconds       \\\hline
        % 475 meters         & 3290  & \\\hline % & 15 seconds       \\\hline
        500 meters         & 592   & 6.000~\%        \\\hline % & 21 seconds       \\\hline
        % 525 meters         & 4035  & \\\hline % & 28 seconds       \\\hline
        550 meters         & 723   & 82.333~\%    \\\hline % & 37 seconds       \\\hline
        % 575 meters         & 4830  & \\\hline % & 49 seconds       \\\hline
        600 meters         & 846   & 99.999~\%   \\\hline % & 62 seconds       \\\hline
        % 625 meters         & 5723  & \\\hline % & 82 seconds       \\\hline
        650 meters         & 994   & 99.999~\%   \\\hline % & 142 seconds      \\\hline
        %675 meters         & 6656  & \\\hline % & 184 seconds      \\\hline
        700 meters         & 1164  & 99.999~\%   \\\hline % & 273 seconds      \\\hline
        % 725 meters         & 7608  & \\\hline % & 375 seconds      \\\hline
        750 meters         & 1349  & 99.999~\%   \\\hline % & 454 seconds      \\\hline
        % 775 meters               & 8696 \\\hline %  & 454 seconds      \\\hline
        800 meters         & 1507  & 99.999~\%   \\\hline %  & 580 seconds      \\\hline
        % 825 meters               & 9837 & \\\hline %  & 708 seconds      \\\hline
        850 meters         & 1714  & 99.999~\%   \\\hline % & 544 seconds      \\\hline
        % 875 meters               & 1101 & \\\hline %5 & 571 seconds      \\\hline
        900 meters         & 1910  & 99.999~\%   \\\hline % & 665 seconds      \\\hline
        % 925 meters               & 1225 & \\\hline %1 & 783 seconds      \\\hline
        950 meters         & 2102  & 99.999~\%   \\\hline % & 916 seconds      \\\hline
        % 975 meters               & 1356 & \\\hline %8 & 1058 seconds     \\\hline
        1000 meters        & 2346  & 99.999~\%   \\\hline % & 1214 seconds     \\\hline
    \end{tabular}
    \caption{Results from the distance threshold experiments.}
    \label{table:cholesky:distance-threshold}
\end{table}


% as the last row in \autoref{table:cholesky:distance-threshold} of 750m gives good results. More aggressive thresholds drastically reduces the total amount of links more, but has the potential of removing links that should have been there.

% 55 \log_{10} \left( d(l_1) \right) - 18


% \subsection{Cholesky decomposition}\label{sec:cholesky}
%In this section we present and describe the Cholesky decomposition, and the problem it creates for our computation time, and how we propose to optimise the decomposition algorithm for our particular needs.
%sec:simulatingvalues
% The cholesky decomposition or cholesky factorization is a matrix decomposition, of a positive-definite matrix, resulting in a lower triangular matrix and its conjugate transpose.

%In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. The Cholesky decomposition is a matrix decomposition, on a \gls{pd-matrix}. The decomposition results in a \gls{lt-matrix} and its \gls{conjugate-transpose}. The Cholesky decomposition is an expensive computation of cubic time complexity, as such we intend to speed up the algorithm. Furthermore since the decomposition requires an \gls{pd-matrix} to work, we choose to verify our auto-correlation matrix before decomposing it, to ensure that the decomposition will run correctly.

% is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,


% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. 
% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition. The Cholesky decomposition is an expensive computation of cubic time complexity and, as such, it needs to be more efficient for our use case. 

%Initially we propose to optimise the algorithm by changing the data structure from a matrix to an ordered map of key-value pairs. The keys will a tuple of links and the value will be the result of the auto-correlation function from \autoref{eq:pathlossautocorrelation}.\medbreak



%, where the pair will be sorted after the link with the largest id, will be the first element in the pair, eg. $l_1.id = 1$ and $l_2.id = 2$ then $key = (l_2, l_1)$. The map must be ordered since the cholesky decomposition uses previous calculated values, to calculate the next.

% shortly introduce cholesky

% our intended improvements


