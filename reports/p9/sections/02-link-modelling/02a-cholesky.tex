\subsection{The Cholesky Decomposition}\label{sec:cholesky}
In \autoref{sec:simulatingvalues}, we utilised the Cholesky decomposition on the covariance matrix in the stochastic shadow fading part. The Cholesky decomposition is a decomposition algorithm for \gls{symmetric}, \gls{pd-matrix} into the product of a \gls{lt-matrix} and its \gls{conjugate-transpose}, and is primarily used for solving systems of linear equations~\cite{Press:2007:NRE:1403886}. In this Section, we will present and describe the Cholesky decomposition, as well as the problems the decomposition creates for our computation time of the stochastic shadow fading part of the link model, as well as possible ways for us to optimise our usage of the Cholesky decomposition. \autoref{algo:cholesky} contains a pseudo code description of the Cholesky decomposition. \medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwResult{The Cholesky decomposition of the input matrix}
    \SetKwFunction{Cholesky}{Cholesky}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\Cholesky{matrix, N}}{
        result $\leftarrow$ empty matrix of size N $\times$ N \;
        \For{n $\leftarrow$ 0, n < N}{
            \For{m $\leftarrow$ 0, m < n + 1}{
                sum $\leftarrow$ 0\;
                \For{i $\leftarrow$ 0, i < m}{
                    sum $\leftarrow$ sum + result$_{n,i} \cdot$ result$_{m,i}$\;
                }
                \If{n = m}{
                    \If{$\text{matrix}_{n,n} - \text{sum} \leq 0$}{
                        throw error; matrix is not positive-definite
                    }

                    results$_{n,m}$ $\leftarrow$ $\sqrt{\text{matrix}_{n,n} - \text{sum}}$\;
                }
                \Else{
                    results$_{n,m} \leftarrow \frac{1}{\text{result}_{m,m}} \cdot (\text{matrix}_{n,m} - sum)$\;
                }
            }
        }
        \KwRet result
    }
    \caption{Cholesky decomposition}
    \label{algo:cholesky}
\end{algorithm}
\medbreak
The first issue we have found with the Cholesky decomposition, or more specifically with the covariance matrix, is that the covariance matrix is not guaranteed to be a \gls{pd-matrix}. The covariance matrix is based on the relation between links in the network, which means that whether the matrix is positive-definite or not is entirely based on the network. To work around this, we have employed a tool call NearPD~\cite{website:nearPD} to transform our covariance matrix into a new matrix that has the positive-definite property, while minimising the Frobenius norm~\cite{website:frobieniusnorm} of the difference between the original and the new matrix. This significantly increases the time required to compute the link model, however, which leads us to our next major issue: The computational time required by the Cholesky decomposition itself. The computational time required for the NearPD tool and the Cholesky decomposition can be seen in \autoref{table:spdcholeskytime}.\smallbreak

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        Nodes & Links   & NearPD          & Cholesky                 \\\hline
        10    & 45      & 3 milliseconds  & \textless{}1 millisecond \\\hline
        20    & 190     & 88 milliseconds & 3 milliseconds           \\\hline
        30    & 435     & 1 seconds       & 34 milliseconds          \\\hline
        40    & 780     & 6 seconds       & 200 milliseconds         \\\hline
        50    & 1225    & 32 seconds      & 720 milliseconds         \\\hline
        60    & 1770    & 113 seconds     & 3 seconds                \\\hline
        70    & 2415    & 285 seconds     & 6 seconds                \\\hline
        80    & 3160    & 584 seconds     & 12 seconds               \\\hline
        90    & 4005    & 22 minutes      & 26 seconds               \\\hline
        100   & 4950    & 35 minutes      & 45 seconds               \\\hline
        110   & 5995    & 75 minutes      & 93 seconds               \\\hline
        120   & 7140    & 127 minutes     & 155 seconds              \\\hline
        130   & 8385    & 235 minutes     & 250 seconds              \\\hline
        140   & 9730    & $\dots$         & $\dots$                  \\\hline
        150   & $\dots$ & $\dots$         & $\dots$                  \\\hline
        160   & $\dots$ & $\dots$         & $\dots$                  \\\hline
        170   & $\dots$ & $\dots$         & $\dots$                  \\\hline
        180   & $\dots$ & $\dots$         & $\dots$                  \\\hline
        190   & $\dots$ & $\dots$         & $\dots$                  \\\hline
        200   & $\dots$ & $\dots$         & $\dots$                  \\\hline
    \end{tabular}
    \caption{Computation time measurement for NearPD and Cholesky decomposition.}
    \label{table:spdcholeskytime}
\end{table}

The Cholesky decomposition has a computational complexity of $O(n^3)$~\citationneeded. With a fully connected network of 1000 nodes, we would have a $\frac{1000(1000+1)}{2} - 1000 = 499500$ (\autoref{eq:lengthoflinks}) unique links, which means that our correlation (and covariance) matrix would be of size $499500 \times 499500$. This is a major issue, as we would like to be able to compute the link model in a relatively short amount of time. To combat this issue, we have two possible solutions: removing links with a distance over a certain threshold, and clustering nodes that are very close to each-other. We present the first solution in this section, and clustering in \autoref{sec:clustering}.\medbreak

\subsubsection{Distance Threshold}\label{sec:distancethreshold}
In \autoref{sec:linkmodel}, we saw that the distance dependent \gls{pathloss} has significantly more importance in the total \gls{pathloss} than the stochastic shadow fading part. In \autoref{eq:pathlossdetermG}, we see that the distance \gls{pathloss} is 92 \acrshort{dbm} for links of 100 meters, and 100.2 \acrshort{dbm} for the diagonal links of $141.42$ meters, and in \autoref{eq:pathlossfadingG}, we see (stochastic) values between $-5.79$ and $10.04$ \acrshort{dbm}. This means that, entirely based on the distance part of the link model, according to the \gls{pep} formula in \autoref{sec:pep}, a distance of 1000 meters, and a transmission power of 26 \acrshort{dbm}, we would have a link \gls{pathloss} of $147$ \acrshort{dbm} (disregarding the stochastic shadow fading part of the \gls{pathloss}), which in turn would mean that the \gls{rssi} on the receiving end of the link would be $26 - 147 = -121$, or equivalent to a packet loss probability of 100 \% (assuming the noise figure and thermal noise is the same as in \autoref{sec:pep}, and the packet size is 160 bits).

% 55 \log_{10} \left( d(l_1) \right) - 18


% \subsection{Cholesky decomposition}\label{sec:cholesky}
%In this section we present and describe the Cholesky decomposition, and the problem it creates for our computation time, and how we propose to optimise the decomposition algorithm for our particular needs.
%sec:simulatingvalues
% The cholesky decomposition or cholesky factorization is a matrix decomposition, of a positive-definite matrix, resulting in a lower triangular matrix and its conjugate transpose.

%In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. The Cholesky decomposition is a matrix decomposition, on a \gls{pd-matrix}. The decomposition results in a \gls{lt-matrix} and its \gls{conjugate-transpose}. The Cholesky decomposition is an expensive computation of cubic time complexity, as such we intend to speed up the algorithm. Furthermore since the decomposition requires an \gls{pd-matrix} to work, we choose to verify our auto-correlation matrix before decomposing it, to ensure that the decomposition will run correctly.

% is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,


% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. 
% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition. The Cholesky decomposition is an expensive computation of cubic time complexity and, as such, it needs to be more efficient for our use case. 

%Initially we propose to optimise the algorithm by changing the data structure from a matrix to an ordered map of key-value pairs. The keys will a tuple of links and the value will be the result of the auto-correlation function from \autoref{eq:pathlossautocorrelation}.\medbreak



%, where the pair will be sorted after the link with the largest id, will be the first element in the pair, eg. $l_1.id = 1$ and $l_2.id = 2$ then $key = (l_2, l_1)$. The map must be ordered since the cholesky decomposition uses previous calculated values, to calculate the next.

% shortly introduce cholesky

% our intended improvements


