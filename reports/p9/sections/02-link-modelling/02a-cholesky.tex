\subsection{The Cholesky Decomposition}\label{sec:cholesky}
In \autoref{sec:simulatingvalues}, we utilised the Cholesky decomposition on the covariance matrix in the stochastic shadow fading part. The Cholesky decomposition is a decomposition algorithm for \gls{symmetric}, \gls{pd-matrix} into the product of a \gls{lt-matrix} and its \gls{conjugate-transpose}, and is primarily used for solving systems of linear equations~\cite{Press:2007:NRE:1403886}. In this Section, we present and describe the Cholesky decomposition, as well as the problems the decomposition creates for our computation time of the stochastic shadow fading part of the link model, as well as possible ways for us to optimise our usage of the Cholesky decomposition. \autoref{algo:cholesky} contains a pseudo code description of the Cholesky decomposition. \medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwResult{The Cholesky decomposition of the input matrix}
    \SetKwFunction{Cholesky}{Cholesky}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\Cholesky{matrix, N}}{
        result $\leftarrow$ empty matrix of size N $\times$ N \;
        \For{n $\leftarrow$ 0, n < N}{
            \For{m $\leftarrow$ 0, m < n + 1}{
                sum $\leftarrow$ 0\;
                \For{i $\leftarrow$ 0, i < m}{
                    sum $\leftarrow$ sum + result$_{n,i} \cdot$ result$_{m,i}$\;
                }
                \If{n = m}{
                    \If{$\text{matrix}_{n,n} - \text{sum} \leq 0$}{
                        throw error; matrix is not positive-definite
                    }

                    results$_{n,m}$ $\leftarrow$ $\sqrt{\text{matrix}_{n,n} - \text{sum}}$\;
                }
                \Else{
                    results$_{n,m} \leftarrow \frac{1}{\text{result}_{m,m}} \cdot (\text{matrix}_{n,m} - sum)$\;
                }
            }
        }
        \KwRet result
    }
    \caption{Cholesky decomposition}
    \label{algo:cholesky}
\end{algorithm}
\medbreak
The first issue we have found with the Cholesky decomposition, or more specifically with the covariance matrix, is that the covariance matrix is not guaranteed to be a \gls{pd-matrix}. The covariance matrix is based on the relation between links in the network, which means that whether the matrix is positive-definite or not is entirely based on the network. To work around this, we have employed a tool call NearPD~\cite{website:nearPD} to transform our covariance matrix into a new matrix that has the positive-definite property, while minimising the Frobenius norm~\cite{website:frobieniusnorm} of the difference between the original and the new matrix. This significantly increases the time required to compute the link model, however, which leads us to our next major issue: The computational time required by the Cholesky decomposition itself. The computational time required for the NearPD tool and the Cholesky decomposition can be seen in \autoref{table:cholesky:spdtime}.\smallbreak

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Nodes & Links & NearPD          & Cholesky                 \\\hline
        10    & 45    & 3 milliseconds  & \textless{}1 millisecond \\\hline
        20    & 190   & 88 milliseconds & 3 milliseconds           \\\hline
        30    & 435   & 1 seconds       & 34 milliseconds          \\\hline
        40    & 780   & 6 seconds       & 200 milliseconds         \\\hline
        50    & 1225  & 32 seconds      & 720 milliseconds         \\\hline
        60    & 1770  & 113 seconds     & 3 seconds                \\\hline
        70    & 2415  & 285 seconds     & 6 seconds                \\\hline
        80    & 3160  & 584 seconds     & 12 seconds               \\\hline
        90    & 4005  & 22 minutes      & 26 seconds               \\\hline
        100   & 4950  & 35 minutes      & 45 seconds               \\\hline
        110   & 5995  & 75 minutes      & 93 seconds               \\\hline
        120   & 7140  & 127 minutes     & 155 seconds              \\\hline
        130   & 8385  & 235 minutes     & 250 seconds              \\\hline
        140   & 9730  & \dots           & 582 seconds              \\\hline
        150   & 11175 & \dots           & 14 minutes               \\\hline
        160   & 12720 & \dots           & 17 minutes               \\\hline
        170   & 14365 & \dots           & 20 minutes               \\\hline
        180   & 16110 & \dots           & 29 minutes               \\\hline
        190   & 17955 & \dots           & 39 minutes               \\\hline
        200   & 19900 & \dots           & 52 minutes               \\\hline
    \end{tabular}
    \caption{Computation time measurement for NearPD and Cholesky decomposition.}
    \label{table:cholesky:spdtime}
\end{table}



\todo[inline]{citation needed}
The Cholesky decomposition has a computational complexity of $O(n^3)$~\citationneeded. With a fully connected network of 1000 nodes, we would have a $\frac{1000(1000+1)}{2} - 1000 = 499500$ (\autoref{eq:lengthoflinks}) unique links, which means that our correlation (and covariance) matrix would be of size $499500 \times 499500$. This is a major issue, as we would like to be able to compute the link model in a relatively short amount of time. To combat this issue, we have two possible solutions: removing links with a distance over a certain threshold, and clustering nodes that are very close to each-other. We present the first solution in this section, and clustering in \autoref{sec:clustering}.\medbreak

\subsubsection{Distance Threshold}\label{sec:distancethreshold}
In \autoref{sec:linkmodel}, we saw that the distance dependent \gls{pathloss} has significantly more importance in the total \gls{pathloss} than the stochastic shadow fading part. In \autoref{eq:pathlossdetermG}, we see that the distance \gls{pathloss} is 92 \acrshort{dbm} for links of 100 meters, and 100.2 \acrshort{dbm} for the diagonal links of $141.42$ meters, and in \autoref{eq:pathlossfadingG}, we see (stochastic) values between $-13.8$ and $6.4$ \acrshort{dbm}. This means that, entirely based on the distance part of the link model, according to the \gls{pep} formula in \autoref{sec:pep}, a distance of 1000 meters, and a transmission power of 26 \acrshort{dbm}, we would have a link \gls{pathloss} of $147$ \acrshort{dbm} (disregarding the stochastic shadow fading part of the \gls{pathloss}), which in turn would mean that the \gls{rssi} on the receiving end of the link would be $26 - 147 = -121$, or equivalent to a packet loss probability of 100 \% (assuming the noise figure and thermal noise is the same as in \autoref{sec:pep}, and the packet size is 160 bits). Hence removing links that are far away can potentially reduce the correlation matrix size significantly.

\begin{table}[H]
    \begin{tabular}{|c|c|c|}\hline
        Distance threshold & Links & Cholesky         \\\hline
        100 meters         & 182   & 3 milliseconds   \\\hline
        125 meters         & 256   & 7 milliseconds   \\\hline
        150 meters         & 360   & 20 milliseconds  \\\hline
        175 meters         & 483   & 46 milliseconds  \\\hline
        200 meters         & 624   & 97 milliseconds  \\\hline
        225 meters         & 773   & 185 milliseconds \\\hline
        250 meters         & 939   & 333 milliseconds \\\hline
        275 meters         & 1157  & 646 milliseconds \\\hline
        300 meters         & 1361  & 1 second         \\\hline
        325 meters         & 1589  & 1 second         \\\hline
        350 meters         & 1822  & 2 seconds        \\\hline
        375 meters         & 2076  & 4 seconds        \\\hline
        400 meters         & 2361  & 6 seconds        \\\hline
        425 meters         & 2642  & 8 seconds        \\\hline
        450 meters         & 2955  & 11 seconds       \\\hline
        475 meters         & 3290  & 15 seconds       \\\hline
        500 meters         & 3652  & 21 seconds       \\\hline
        525 meters         & 4035  & 28 seconds       \\\hline
        550 meters         & 4389  & 37 seconds       \\\hline
        575 meters         & 4830  & 49 seconds       \\\hline
        600 meters         & 5260  & 62 seconds       \\\hline
        625 meters         & 5723  & 82 seconds       \\\hline
        650 meters         & 6163  & 142 seconds      \\\hline
        675 meters         & 6656  & 184 seconds      \\\hline
        700 meters         & 7105  & 273 seconds      \\\hline
        725 meters         & 7608  & 375 seconds      \\\hline
        750 meters         & 8127  & 454 seconds      \\\hline
        % 775m               & 8696  & 454 seconds      \\\hline
        % 800m               & 9240  & 580 seconds      \\\hline
        % 825m               & 9837  & 708 seconds      \\\hline
        % 850m               & 10421 & 544 seconds      \\\hline
        % 875m               & 11015 & 571 seconds      \\\hline
        % 900m               & 11611 & 665 seconds      \\\hline
        % 925m               & 12251 & 783 seconds      \\\hline
        % 950m               & 12934 & 916 seconds      \\\hline
        % 975m               & 13568 & 1058 seconds     \\\hline
        % 1000m              & 14232 & 1214 seconds     \\\hline
    \end{tabular}
    \caption{Results from the distance threshold experiments.}
    \label{table:cholesky:distance-threshold}
\end{table}

The experiment consisted of generating 1000 random nodes, create links from those nodes based on their location, iteratively scale the maximum allowed distance of the links (distance threshold) and measure the time used to do the cholesky computation. The results can be read in \autoref{table:cholesky:distance-threshold}. The content of each column are as followed:

\begin{description}
    \item[Distance threshold:] The maximum distance of the link.
    \item[Links:] The total amount of links.
    \item[Cholesky:] The time used for the cholesky computation.
\end{description}


The results shows that limiting the maximum distance can drastically reduce the total amount of links, even if the threshold is set relative high. When comparing the link counts with \autoref{table:cholesky:spdtime}, the improvements are clear. When the distance threshold is set to 750 meters, it results in 8127 links from 1000 nodes. Without the distance threshold optimisations, 130 nodes would result in 8385 links which is very close. This optimisation allows for larger networks, by drastically reducing the total links.


% as the last row in \autoref{table:cholesky:distance-threshold} of 750m gives good results. More aggressive thresholds drastically reduces the total amount of links more, but has the potential of removing links that should have been there.

% 55 \log_{10} \left( d(l_1) \right) - 18


% \subsection{Cholesky decomposition}\label{sec:cholesky}
%In this section we present and describe the Cholesky decomposition, and the problem it creates for our computation time, and how we propose to optimise the decomposition algorithm for our particular needs.
%sec:simulatingvalues
% The cholesky decomposition or cholesky factorization is a matrix decomposition, of a positive-definite matrix, resulting in a lower triangular matrix and its conjugate transpose.

%In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. The Cholesky decomposition is a matrix decomposition, on a \gls{pd-matrix}. The decomposition results in a \gls{lt-matrix} and its \gls{conjugate-transpose}. The Cholesky decomposition is an expensive computation of cubic time complexity, as such we intend to speed up the algorithm. Furthermore since the decomposition requires an \gls{pd-matrix} to work, we choose to verify our auto-correlation matrix before decomposing it, to ensure that the decomposition will run correctly.

% is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,


% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. 
% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition. The Cholesky decomposition is an expensive computation of cubic time complexity and, as such, it needs to be more efficient for our use case. 

%Initially we propose to optimise the algorithm by changing the data structure from a matrix to an ordered map of key-value pairs. The keys will a tuple of links and the value will be the result of the auto-correlation function from \autoref{eq:pathlossautocorrelation}.\medbreak



%, where the pair will be sorted after the link with the largest id, will be the first element in the pair, eg. $l_1.id = 1$ and $l_2.id = 2$ then $key = (l_2, l_1)$. The map must be ordered since the cholesky decomposition uses previous calculated values, to calculate the next.

% shortly introduce cholesky

% our intended improvements


