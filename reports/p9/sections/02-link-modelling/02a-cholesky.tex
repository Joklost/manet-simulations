\subsection{The Cholesky Decomposition}\label{sec:cholesky}
In \autoref{sec:simulatingvalues}, we utilised the Cholesky decomposition on the covariance matrix in the stochastic shadow fading part. The Cholesky decomposition is a decomposition algorithm for \gls{symmetric}, \gls{pd-matrix} into the product of a \gls{lt-matrix} and its \gls{conjugate-transpose}, and is primarily used for solving systems of linear equations~\cite{Press:2007:NRE:1403886}. In this Section, we will present and describe the Cholesky decomposition, as well as the problems the decomposition creates for our computation time of the stochastic shadow fading part of the link model, as well as possible ways for us to optimise our usage of the Cholesky decomposition. \autoref{algo:cholesky} contains a pseudo code description of the Cholesky decomposition. \medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwResult{The Cholesky decomposition of the input matrix}
    \SetKwFunction{Cholesky}{Cholesky}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\Cholesky{matrix, N}}{
        result $\leftarrow$ empty matrix of size N $\times$ N \;
       \For{n $\leftarrow$ 0, n < N}{
            \For{m $\leftarrow$ 0, m < n + 1}{
                sum $\leftarrow$ 0\;
                \For{i $\leftarrow$ 0, i < m}{
                    sum $\leftarrow$ sum + result$_{n,i} \cdot$ result$_{m,i}$\;
                }
                \If{n = m}{
                    \If{$\text{matrix}_{n,n} - \text{sum} \leq 0$}{
                        throw error; matrix is not positive-definite
                    }

                    results$_{n,m}$ $\leftarrow$ $\sqrt{\text{matrix}_{n,n} - \text{sum}}$\;
                }
                \Else{
                    results$_{n,m} \leftarrow \frac{1}{\text{result}_{m,m}} \cdot (\text{matrix}_{n,m} - sum)$\;
                }
            }
        }
        \KwRet result
    }
    \caption{Cholesky decomposition}
    \label{algo:cholesky}
\end{algorithm}
\medbreak
The first issue we have found with the Cholesky decomposition, or more specifically with the covariance matrix, is that the covariance matrix is not guaranteed to be a \gls{pd-matrix}. The covariance matrix is based on the relation between links in the network, which means that whether the matrix is positive-definite or not is entirely based on the network. To work around this, we have employed a tool~\cite{website:nearestspd} to transform our covariance matrix into a new matrix that has the positive-definite property, while minimising the Frobenius norm~\cite{website:frobieniusnorm} of the difference between the original and the new matrix. This significantly increases the time required to compute the link model, however, which leads us to our next major issue: The computational time required by the Cholesky decomposition itself. \smallbreak
The Cholesky decomposition has a computational complexity of $O(n^3)$~\citationneeded. With a fully connected network of 1000 nodes, we would have a $\frac{1000(1000+1)}{2} - 1000 = 499500$ (\autoref{eq:lengthoflinks}) unique links, which means that our correlation (and covariance) matrix would be of size $499500 \times 499500$. This is a major issue, as we would like to be able to compute the link model in a relatively short amount of time. To combat this issue, we have two possible solutions: removing links with a distance over a certain threshold, and clustering nodes that are very close to each-other. We will present the first solution in this Section, and clustering in \autoref{sec:clustering}.\medbreak

\subsubsection{Distance Threshold}\label{sec:distancethreshold}
In \autoref{sec:linkmodel}, we saw that the distance dependent \gls{pathloss} has significantly more importance in the total \gls{pathloss} than the stochastic shadow fading part. In \autoref{eq:pathlossdetermG}, we see that the distance \gls{pathloss} is 92 \acrshort{dbm} for links of 100 meters, and 100.2 \acrshort{dbm} for the diagonal links of $141.42$ meters, and in \autoref{eq:pathlossfadingG}, we see (stochastic) values between $-5.79$ and $10.04$ \acrshort{dbm}. This means that, entirely based on the distance part of the link model, according to the \gls{pep} formula in \autoref{sec:pep}, a distance of 1000 meters, and a transmission power of 26 \acrshort{dbm}, we would have a link \gls{pathloss} of $147$ \acrshort{dbm} (disregarding the stochastic shadow fading part of the \gls{pathloss}), which in turn would mean that the \gls{rssi} on the receiving end of the link would be $26 - 147 = -121$, or equivalent to a packet loss probability of 100 \% (assuming the noise figure and thermal noise is the same as in the \gls{pep} Section, and the packet size is 160 bits).

% 55 \log_{10} \left( d(l_1) \right) - 18


% \subsection{Cholesky decomposition}\label{sec:cholesky}
%In this section we present and describe the Cholesky decomposition, and the problem it creates for our computation time, and how we propose to optimise the decomposition algorithm for our particular needs.
%sec:simulatingvalues
% The cholesky decomposition or cholesky factorization is a matrix decomposition, of a positive-definite matrix, resulting in a lower triangular matrix and its conjugate transpose.

%In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. The Cholesky decomposition is a matrix decomposition, on a \gls{pd-matrix}. The decomposition results in a \gls{lt-matrix} and its \gls{conjugate-transpose}. The Cholesky decomposition is an expensive computation of cubic time complexity, as such we intend to speed up the algorithm. Furthermore since the decomposition requires an \gls{pd-matrix} to work, we choose to verify our auto-correlation matrix before decomposing it, to ensure that the decomposition will run correctly.

% is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,


% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition in \autoref{eq:pathlossstoch}. 
% In \autoref{sec:linkmodel} we utilise the Cholesky decomposition. The Cholesky decomposition is an expensive computation of cubic time complexity and, as such, it needs to be more efficient for our use case. 

%Initially we propose to optimise the algorithm by changing the data structure from a matrix to an ordered map of key-value pairs. The keys will a tuple of links and the value will be the result of the auto-correlation function from \autoref{eq:pathlossautocorrelation}.\medbreak



%, where the pair will be sorted after the link with the largest id, will be the first element in the pair, eg. $l_1.id = 1$ and $l_2.id = 2$ then $key = (l_2, l_1)$. The map must be ordered since the cholesky decomposition uses previous calculated values, to calculate the next.

% shortly introduce cholesky

% our intended improvements



%\begin{table}[H]
%    \centering
%    \begin{tabular}{|l|l|l|l|}
%    \hline
%    Nodes & Links & NearestSPD & Cholesky        \\\hline
%    10    & 45    & 43 ms      & \textless{}1 ms \\\hline
%    20    & 190   & 9 s        & 2 ms            \\\hline
%    30    & 435   & 144 s      & 26 ms           \\\hline
%    \end{tabular}
%    \caption{Computation time measurement for NearestSPD and Cholesky decomposition.}
%    \label{table:spdcholeskytime}
%\end{table}

\todo[inline]{Add table showing computation time for different sized matrices. with/without nearest SPD?}